{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a5ed54c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "25f52830",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('Breakout-v0')\n",
    "height, width, channels = env.observation_space.shape\n",
    "actions = env.action_space.n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b173d091",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['NOOP', 'FIRE', 'RIGHT', 'LEFT']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.unwrapped.get_action_meanings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cccc85c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode:1 Score:1.0\n",
      "Episode:2 Score:0.0\n",
      "Episode:3 Score:1.0\n",
      "Episode:4 Score:0.0\n",
      "Episode:5 Score:0.0\n"
     ]
    }
   ],
   "source": [
    "episodes = 5\n",
    "for episode in range(1, episodes+1):\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    score = 0 \n",
    "    \n",
    "    while not done:\n",
    "        env.render()\n",
    "        action = random.choice([0,1,2,3])\n",
    "        n_state, reward, done, info = env.step(action)\n",
    "        score+=reward\n",
    "    print('Episode:{} Score:{}'.format(episode, score))\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fb0a7acb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import time\n",
    "import random\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "from skimage.color import rgb2gray\n",
    "from skimage.transform import resize\n",
    "\n",
    "from tensorflow.keras.layers import Conv2D, Dense, Flatten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "aa0ed516",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 상태가 입력, 큐함수가 출력인 인공신경망 생성\n",
    "class DQN(tf.keras.Model):\n",
    "    def __init__(self, action_size, state_size):\n",
    "        super(DQN, self).__init__()\n",
    "        self.conv1 = Conv2D(32, (8, 8), strides=(4, 4), activation='relu',\n",
    "                            input_shape=state_size)\n",
    "        self.conv2 = Conv2D(64, (4, 4), strides=(2, 2), activation='relu')\n",
    "        self.conv3 = Conv2D(64, (3, 3), strides=(1, 1), activation='relu')\n",
    "        self.flatten = Flatten()\n",
    "        self.fc = Dense(512, activation='relu')\n",
    "        self.fc_out = Dense(action_size)\n",
    "\n",
    "    def call(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.conv3(x)\n",
    "        x = self.flatten(x)\n",
    "        x = self.fc(x)\n",
    "        q = self.fc_out(x)\n",
    "        return q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "07113200",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 브레이크아웃 예제에서의 DQN 에이전트\n",
    "class DQNAgent:\n",
    "    def __init__(self, action_size, state_size, model_path):\n",
    "        self.render = False\n",
    "\n",
    "        # 상태와 행동의 크기 정의\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "\n",
    "        self.epsilon = 0.02\n",
    "\n",
    "        # 모델과 타깃 모델 생성\n",
    "        self.model = DQN(action_size, state_size)\n",
    "        self.model.load_weights(model_path)\n",
    "\n",
    "    # 입실론 탐욕 정책으로 행동 선택\n",
    "    def get_action(self, history):\n",
    "        history = np.float32(history / 255.0)\n",
    "        if np.random.rand() <= self.epsilon:\n",
    "            return random.randrange(self.action_size)\n",
    "        else:\n",
    "            q_value = self.model(history)\n",
    "            return np.argmax(q_value[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e96627fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pre_processing(observe):\n",
    "    processed_observe = np.uint8(\n",
    "        resize(rgb2gray(observe), (84, 84), mode='constant') * 255)\n",
    "    return processed_observe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b115ceff",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-14-1d8b30f26ca7>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      8\u001b[0m     \u001b[0maction_size\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m3\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m     \u001b[0mmodel_path\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'./save_model/trained/model'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m     \u001b[0magent\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mDQNAgent\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maction_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstate_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     11\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m     \u001b[1;31m# 불필요한 행동을 없애주기 위한 딕셔너리 선언\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-12-79a17d7c15e4>\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, action_size, state_size, model_path)\u001b[0m\n\u001b[0;32m     12\u001b[0m         \u001b[1;31m# 모델과 타깃 모델 생성\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mDQN\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maction_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstate_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 14\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload_weights\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     15\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m     \u001b[1;31m# 입실론 탐욕 정책으로 행동 선택\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mload_weights\u001b[1;34m(self, filepath, by_name, skip_mismatch, options)\u001b[0m\n\u001b[0;32m   2174\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2175\u001b[0m       \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2176\u001b[1;33m         \u001b[0mpy_checkpoint_reader\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mNewCheckpointReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2177\u001b[0m         \u001b[0msave_format\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'tf'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2178\u001b[0m       \u001b[1;32mexcept\u001b[0m \u001b[0merrors_impl\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataLossError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\training\\py_checkpoint_reader.py\u001b[0m in \u001b[0;36mNewCheckpointReader\u001b[1;34m(filepattern)\u001b[0m\n\u001b[0;32m     93\u001b[0m   \"\"\"\n\u001b[0;32m     94\u001b[0m   \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 95\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mCheckpointReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_bytes\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepattern\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     96\u001b[0m   \u001b[1;31m# TODO(b/143319754): Remove the RuntimeError casting logic once we resolve the\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     97\u001b[0m   \u001b[1;31m# issue with throwing python exceptions from C++.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # 환경 세팅\n",
    "    env = gym.make(\"BreakoutDeterministic-v4\")\n",
    "    render = True\n",
    "\n",
    "    # 테스트를 위한 에이전트 생성\n",
    "    state_size = (84, 84, 4)\n",
    "    action_size = 3\n",
    "    model_path = './save_model/trained/model'\n",
    "    agent = DQNAgent(action_size, state_size, model_path)\n",
    "\n",
    "    # 불필요한 행동을 없애주기 위한 딕셔너리 선언\n",
    "    action_dict = {0:1, 1:2, 2:3, 3:3}\n",
    "\n",
    "    num_episode = 10\n",
    "    for e in range(num_episode):\n",
    "        done = False\n",
    "        dead = False\n",
    "\n",
    "        score, start_life = 0, 5\n",
    "        # env 초기화\n",
    "        observe = env.reset()\n",
    "\n",
    "        # 랜덤으로 뽑힌 값 만큼의 프레임동안 움직이지 않음\n",
    "        for _ in range(random.randint(1, 30)):\n",
    "            observe, _, _, _ = env.step(1)\n",
    "\n",
    "        # 프레임을 전처리 한 후 4개의 상태를 쌓아서 입력값으로 사용.\n",
    "        state = pre_processing(observe)\n",
    "        history = np.stack([state, state, state, state], axis=2)\n",
    "        history = np.reshape([history], (1, 84, 84, 4))\n",
    "\n",
    "        while not done:\n",
    "            if render:\n",
    "                env.render()\n",
    "                time.sleep(0.05)\n",
    "\n",
    "            # 바로 전 history를 입력으로 받아 행동을 선택\n",
    "            action = agent.get_action(history)\n",
    "            # 1: 정지, 2: 왼쪽, 3: 오른쪽\n",
    "            real_action = action_dict[action]\n",
    "            \n",
    "            # 죽었을 때 시작하기 위해 발사 행동을 함\n",
    "            if dead:\n",
    "                action, real_action, dead = 0, 1, False\n",
    "\n",
    "            # 선택한 행동으로 환경에서 한 타임스텝 진행\n",
    "            observe, reward, done, info = env.step(real_action)\n",
    "            # 각 타임스텝마다 상태 전처리\n",
    "            next_state = pre_processing(observe)\n",
    "            next_state = np.reshape([next_state], (1, 84, 84, 1))\n",
    "            next_history = np.append(next_state, history[:, :, :, :3], axis=3)\n",
    "\n",
    "            if start_life > info['ale.lives']:\n",
    "                dead, start_life = True, info['ale.lives']\n",
    "\n",
    "            score += reward\n",
    "\n",
    "            if dead:\n",
    "                history = np.stack((next_state, next_state,\n",
    "                                    next_state, next_state), axis=2)\n",
    "                history = np.reshape([history], (1, 84, 84, 4))\n",
    "            else:\n",
    "                history = next_history\n",
    "\n",
    "            if done:\n",
    "                # 각 에피소드 당 테스트 정보를 기록\n",
    "                print(\"episode: {:3d} | score : {:4.1f}\".format(e, score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5740fd1a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
