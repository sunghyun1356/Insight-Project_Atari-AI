{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e460a6ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\edr2\\anaconda3\\envs\\game\\lib\\site-packages\\ale_py\\roms\\utils.py:90: DeprecationWarning: SelectableGroups dict interface is deprecated. Use select.\n",
      "  for external in metadata.entry_points().get(self.group, []):\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import time\n",
    "import random\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "from skimage.color import rgb2gray\n",
    "from skimage.transform import resize\n",
    "\n",
    "from tensorflow.keras.layers import Conv2D, Flatten, Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "676ad0be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ActorCritic 인공신경망\n",
    "class ActorCritic(tf.keras.Model):\n",
    "    def __init__(self, action_size, state_size):\n",
    "        super(ActorCritic, self).__init__()\n",
    "\n",
    "        self.conv1 = Conv2D(32, (8, 8), strides=(4, 4), activation='relu',\n",
    "                            input_shape=state_size)\n",
    "        self.conv2 = Conv2D(64, (4, 4), strides=(2, 2), activation='relu')\n",
    "        self.conv3 = Conv2D(64, (3, 3), strides=(1, 1), activation='relu')\n",
    "        self.flatten = Flatten()\n",
    "        self.shared_fc = Dense(512, activation='relu')\n",
    "\n",
    "        self.policy = Dense(action_size, activation='linear')\n",
    "        self.value = Dense(1, activation='linear')\n",
    "\n",
    "    def call(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.flatten(x)\n",
    "        x = self.shared_fc(x)\n",
    "\n",
    "        policy = self.policy(x)\n",
    "        value = self.value(x)\n",
    "        return policy, value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4f69be57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 브레이크아웃에서의 테스트를 위한 A3C 에이전트 클래스\n",
    "class A3CTestAgent:\n",
    "    def __init__(self, action_size, state_size, model_path):\n",
    "        self.action_size = action_size\n",
    "\n",
    "        self.model = ActorCritic(action_size, state_size)\n",
    "        self.model.load_weights(model_path)\n",
    "\n",
    "    def get_action(self, history):\n",
    "        history = np.float32(history / 255.)\n",
    "        policy = self.model(history)[0][0]\n",
    "        policy = tf.nn.softmax(policy)\n",
    "        action_index = np.random.choice(self.action_size, 1, p=policy.numpy())[0]\n",
    "        return action_index, policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3d962ea3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pre_processing(observe):\n",
    "    processed_observe = np.uint8(\n",
    "        resize(rgb2gray(observe), (84, 84), mode='constant') * 255)\n",
    "    return processed_observe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "772f4615",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\edr2\\anaconda3\\envs\\game\\lib\\site-packages\\gym\\envs\\atari\\environment.py:257: UserWarning: \u001b[33mWARN: We strongly suggest supplying `render_mode` when constructing your environment, e.g., gym.make(ID, render_mode='human'). Using `render_mode` provides access to proper scaling, audio support, and proper framerates.\u001b[0m\n",
      "  \"We strongly suggest supplying `render_mode` when \"\n",
      "C:\\Users\\edr2\\anaconda3\\envs\\game\\lib\\site-packages\\pyglet\\image\\codecs\\wic.py:289: UserWarning: [WinError -2147417850] 스레드 모드가 설정된 후에는 바꿀 수 없습니다\n",
      "  warnings.warn(str(err))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode:   0 | score : 82.0\n",
      "episode:   1 | score : 369.0\n",
      "episode:   2 | score : 407.0\n",
      "episode:   3 | score : 273.0\n",
      "episode:   4 | score : 367.0\n",
      "episode:   5 | score : 209.0\n",
      "episode:   6 | score : 392.0\n",
      "episode:   7 | score : 264.0\n",
      "episode:   8 | score : 319.0\n",
      "episode:   9 | score : 72.0\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # 테스트를 위한 환경, 모델 생성\n",
    "    env = gym.make(\"BreakoutDeterministic-v4\")\n",
    "    state_size = (84, 84, 4)\n",
    "    action_size = 3\n",
    "    model_path = './save_model/trained/model'\n",
    "    render = True\n",
    "\n",
    "    agent = A3CTestAgent(action_size, state_size, model_path)\n",
    "    action_dict = {0:1, 1:2, 2:3, 3:3}\n",
    "\n",
    "    num_episode = 10\n",
    "    for e in range(num_episode):\n",
    "        done = False\n",
    "        dead = False\n",
    "\n",
    "        score, start_life = 0, 5\n",
    "        observe = env.reset()\n",
    "\n",
    "        # 랜덤으로 뽑힌 값 만큼의 프레임동안 움직이지 않음\n",
    "        for _ in range(random.randint(1, 30)):\n",
    "            observe, _, _, _ = env.step(1)\n",
    "\n",
    "        # 프레임을 전처리 한 후 4개의 상태를 쌓아서 입력값으로 사용.\n",
    "        state = pre_processing(observe)\n",
    "        history = np.stack([state, state, state, state], axis=2)\n",
    "        history = np.reshape([history], (1, 84, 84, 4))\n",
    "\n",
    "        while not done:\n",
    "            if render:\n",
    "                env.render()\n",
    "                time.sleep(0.05)\n",
    "\n",
    "            # 정책 확률에 따라 행동을 선택\n",
    "            action, policy = agent.get_action(history)\n",
    "            # 1: 정지, 2: 왼쪽, 3: 오른쪽\n",
    "            real_action = action_dict[action]\n",
    "            # 죽었을 때 시작하기 위해 발사 행동을 함\n",
    "            if dead:\n",
    "                action, real_action, dead = 0, 1, False\n",
    "\n",
    "            # 선택한 행동으로 환경에서 한 타임스텝 진행\n",
    "            observe, reward, done, info = env.step(real_action)\n",
    "\n",
    "            # 각 타임스텝마다 상태 전처리\n",
    "            next_state = pre_processing(observe)\n",
    "            next_state = np.reshape([next_state], (1, 84, 84, 1))\n",
    "            next_history = np.append(next_state, history[:, :, :, :3], axis=3)\n",
    "\n",
    "            if start_life > info['lives']:\n",
    "                dead, start_life = True, info['lives']\n",
    "\n",
    "            score += reward\n",
    "\n",
    "            if dead:\n",
    "                history = np.stack((next_state, next_state,\n",
    "                                    next_state, next_state), axis=2)\n",
    "                history = np.reshape([history], (1, 84, 84, 4))\n",
    "            else:\n",
    "                history = next_history\n",
    "\n",
    "            if done:\n",
    "                # 각 에피소드 당 학습 정보를 기록\n",
    "                print(\"episode: {:3d} | score : {:4.1f}\".format(e, score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc644ee7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "game",
   "language": "python",
   "name": "myenvname"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
