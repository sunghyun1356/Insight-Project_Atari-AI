{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eab87124",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import gym\n",
    "import time\n",
    "import threading\n",
    "import random\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "from skimage.color import rgb2gray\n",
    "from skimage.transform import resize\n",
    "\n",
    "from tensorflow.compat.v1.train import AdamOptimizer\n",
    "from tensorflow.keras.layers import Conv2D, Flatten, Dense\n",
    "\n",
    "# 멀티쓰레딩을 위한 글로벌 변수\n",
    "global episode, score_avg, score_max\n",
    "episode, score_avg, score_max = 0, 0, 0\n",
    "num_episode = 8000000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "728759bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ActorCritic 인공신경망\n",
    "class ActorCritic(tf.keras.Model):\n",
    "    def __init__(self, action_size, state_size):\n",
    "        super(ActorCritic, self).__init__()\n",
    "\n",
    "        self.conv1 = Conv2D(32, (8, 8), strides=(4, 4), activation='relu',\n",
    "                            input_shape=state_size)\n",
    "        self.conv2 = Conv2D(64, (4, 4), strides=(2, 2), activation='relu')\n",
    "        self.conv3 = Conv2D(64, (3, 3), strides=(1, 1), activation='relu')\n",
    "        self.flatten = Flatten()\n",
    "        self.shared_fc = Dense(512, activation='relu')\n",
    "\n",
    "        self.policy = Dense(action_size, activation='linear')\n",
    "        self.value = Dense(1, activation='linear')\n",
    "\n",
    "    def call(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.flatten(x)\n",
    "        x = self.shared_fc(x)\n",
    "\n",
    "        policy = self.policy(x)\n",
    "        value = self.value(x)\n",
    "        return policy, value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e944c9ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 브레이크아웃에서의 A3CAgent 클래스 (글로벌신경망)\n",
    "class A3CAgent():\n",
    "    def __init__(self, action_size, env_name):\n",
    "        self.env_name = env_name\n",
    "        # 상태와 행동의 크기 정의\n",
    "        self.state_size = (84, 84, 4)\n",
    "        self.action_size = action_size\n",
    "        # A3C 하이퍼파라미터\n",
    "        self.discount_factor = 0.99\n",
    "        self.no_op_steps = 30\n",
    "        self.lr = 1e-4\n",
    "        # 쓰레드의 갯수\n",
    "        self.threads = 16\n",
    "\n",
    "        # 글로벌 인공신경망 생성\n",
    "        self.global_model = ActorCritic(self.action_size, self.state_size)\n",
    "        # 글로벌 인공신경망의 가중치 초기화\n",
    "        self.global_model.build(tf.TensorShape((None, *self.state_size)))\n",
    "\n",
    "        # 인공신경망 업데이트하는 옵티마이저 함수 생성\n",
    "        self.optimizer = AdamOptimizer(self.lr, use_locking=True)\n",
    "\n",
    "        # 텐서보드 설정\n",
    "        self.writer = tf.summary.create_file_writer('summary/breakout_a3c')\n",
    "        # 학습된 글로벌신경망 모델을 저장할 경로 설정\n",
    "        self.model_path = os.path.join(os.getcwd(), 'save_model', 'model')\n",
    "\n",
    "    # 쓰레드를 만들어 학습을 하는 함수\n",
    "    def train(self):\n",
    "        # 쓰레드 수 만큼 Runner 클래스 생성\n",
    "        runners = [Runner(self.action_size, self.state_size,\n",
    "                          self.global_model, self.optimizer,\n",
    "                          self.discount_factor, self.env_name,\n",
    "                          self.writer) for i in range(self.threads)]\n",
    "\n",
    "        # 각 쓰레드 시정\n",
    "        for i, runner in enumerate(runners):\n",
    "            print(\"Start worker #{:d}\".format(i))\n",
    "            runner.start()\n",
    "\n",
    "        # 10분 (600초)에 한 번씩 모델을 저장\n",
    "        while True:\n",
    "            self.global_model.save_weights(self.model_path, save_format=\"tf\")\n",
    "            time.sleep(60 * 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99c83c0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 액터러너 클래스 (쓰레드)\n",
    "class Runner(threading.Thread):\n",
    "    global_episode = 0\n",
    "\n",
    "    def __init__(self, action_size, state_size, global_model,\n",
    "                 optimizer, discount_factor, env_name, writer):\n",
    "        threading.Thread.__init__(self)\n",
    "\n",
    "        # A3CAgent 클래스에서 넘겨준 하이준 파라미터 설정\n",
    "        self.action_size = action_size\n",
    "        self.state_size = state_size\n",
    "        self.global_model = global_model\n",
    "        self.optimizer = optimizer\n",
    "        self.discount_factor = discount_factor\n",
    "\n",
    "        self.states, self.actions, self.rewards = [], [], []\n",
    "\n",
    "        # 환경, 로컬신경망, 텐서보드 생성\n",
    "        self.local_model = ActorCritic(action_size, state_size)\n",
    "        self.env = gym.make(env_name)\n",
    "        self.writer = writer\n",
    "\n",
    "        # 학습 정보를 기록할 변수\n",
    "        self.avg_p_max = 0\n",
    "        self.avg_loss = 0\n",
    "        # k-타임스텝 값 설정\n",
    "        self.t_max = 20\n",
    "        self.t = 0\n",
    "        # 불필요한 행동을 줄여주기 위한 dictionary\n",
    "        self.action_dict = {0:1, 1:2, 2:3, 3:3}\n",
    "\n",
    "    # 텐서보드에 학습 정보를 기록\n",
    "    def draw_tensorboard(self, score, step, e):\n",
    "        avg_p_max = self.avg_p_max / float(step)\n",
    "        with self.writer.as_default():\n",
    "            tf.summary.scalar('Total Reward/Episode', score, step=e)\n",
    "            tf.summary.scalar('Average Max Prob/Episode', avg_p_max, step=e)\n",
    "            tf.summary.scalar('Duration/Episode', step, step=e)\n",
    "\n",
    "    # 정책신경망의 출력을 받아 확률적으로 행동을 선택\n",
    "    def get_action(self, history):\n",
    "        history = np.float32(history / 255.)\n",
    "        policy = self.local_model(history)[0][0]\n",
    "        policy = tf.nn.softmax(policy)\n",
    "        action_index = np.random.choice(self.action_size, 1, p=policy.numpy())[0]\n",
    "        return action_index, policy\n",
    "\n",
    "    # 샘플을 저장\n",
    "    def append_sample(self, history, action, reward):\n",
    "        self.states.append(history)\n",
    "        act = np.zeros(self.action_size)\n",
    "        act[action] = 1\n",
    "        self.actions.append(act)\n",
    "        self.rewards.append(reward)\n",
    "\n",
    "    # k-타임스텝의 prediction 계산\n",
    "    def discounted_prediction(self, rewards, done):\n",
    "        discounted_prediction = np.zeros_like(rewards)\n",
    "        running_add = 0\n",
    "\n",
    "        if not done:\n",
    "            # value function\n",
    "            last_state = np.float32(self.states[-1] / 255.)\n",
    "            running_add = self.local_model(last_state)[-1][0].numpy()\n",
    "\n",
    "        for t in reversed(range(0, len(rewards))):\n",
    "            running_add = running_add * self.discount_factor + rewards[t]\n",
    "            discounted_prediction[t] = running_add\n",
    "        return discounted_prediction\n",
    "\n",
    "    # 저장된 샘플들로 A3C의 오류함수를 계산\n",
    "    def compute_loss(self, done):\n",
    "\n",
    "        discounted_prediction = self.discounted_prediction(self.rewards, done)\n",
    "        discounted_prediction = tf.convert_to_tensor(discounted_prediction[:, None],\n",
    "                                                     dtype=tf.float32)\n",
    "\n",
    "        states = np.zeros((len(self.states), 84, 84, 4))\n",
    "\n",
    "        for i in range(len(self.states)):\n",
    "            states[i] = self.states[i]\n",
    "        states = np.float32(states / 255.)\n",
    "\n",
    "        policy, values = self.local_model(states)\n",
    "\n",
    "        # 가치 신경망 업데이트\n",
    "        advantages = discounted_prediction - values\n",
    "        critic_loss = 0.5 * tf.reduce_sum(tf.square(advantages))\n",
    "\n",
    "        # 정책 신경망 업데이트\n",
    "        action = tf.convert_to_tensor(self.actions, dtype=tf.float32)\n",
    "        policy_prob = tf.nn.softmax(policy)\n",
    "        action_prob = tf.reduce_sum(action * policy_prob, axis=1, keepdims=True)\n",
    "        cross_entropy = - tf.math.log(action_prob + 1e-10)\n",
    "        actor_loss = tf.reduce_sum(cross_entropy * tf.stop_gradient(advantages))\n",
    "\n",
    "        entropy = tf.reduce_sum(policy_prob * tf.math.log(policy_prob + 1e-10), axis=1)\n",
    "        entropy = tf.reduce_sum(entropy)\n",
    "        actor_loss += 0.01 * entropy\n",
    "\n",
    "        total_loss = 0.5 * critic_loss + actor_loss\n",
    "\n",
    "        return total_loss\n",
    "\n",
    "    # 로컬신경망을 통해 그레이디언트를 계산하고, 글로벌 신경망을 계산된 그레이디언트로 업데이트\n",
    "    def train_model(self, done):\n",
    "\n",
    "        global_params = self.global_model.trainable_variables\n",
    "        local_params = self.local_model.trainable_variables\n",
    "\n",
    "        with tf.GradientTape() as tape:\n",
    "            total_loss = self.compute_loss(done)\n",
    "\n",
    "        # 로컬신경망의 그레이디언트 계산\n",
    "        grads = tape.gradient(total_loss, local_params)\n",
    "        # 안정적인 학습을 위한 그레이디언트 클리핑\n",
    "        grads, _ = tf.clip_by_global_norm(grads, 40.0)\n",
    "        # 로컬신경망의 오류함수를 줄이는 방향으로 글로벌신경망을 업데이트\n",
    "        self.optimizer.apply_gradients(zip(grads, global_params))\n",
    "        # 로컬신경망의 가중치를 글로벌신경망의 가중치로 업데이트\n",
    "        self.local_model.set_weights(self.global_model.get_weights())\n",
    "        # 업데이트 후 저장된 샘플 초기화\n",
    "        self.states, self.actions, self.rewards = [], [], []\n",
    "\n",
    "    def run(self):\n",
    "        # 액터러너끼리 공유해야하는 글로벌 변수\n",
    "        global episode, score_avg, score_max\n",
    "\n",
    "        step = 0\n",
    "        while episode < num_episode:\n",
    "            done = False\n",
    "            dead = False\n",
    "\n",
    "            score, start_life = 0, 5\n",
    "            observe = self.env.reset()\n",
    "\n",
    "            # 랜덤으로 뽑힌 값 만큼의 프레임동안 움직이지 않음\n",
    "            for _ in range(random.randint(1, 30)):\n",
    "                observe, _, _, _ = self.env.step(1)\n",
    "\n",
    "            # 프레임을 전처리 한 후 4개의 상태를 쌓아서 입력값으로 사용.\n",
    "            state = pre_processing(observe)\n",
    "            history = np.stack([state, state, state, state], axis=2)\n",
    "            history = np.reshape([history], (1, 84, 84, 4))\n",
    "\n",
    "            while not done:\n",
    "                step += 1\n",
    "                self.t += 1\n",
    "\n",
    "                # 정책 확률에 따라 행동을 선택\n",
    "                action, policy = self.get_action(history)\n",
    "                # 1: 정지, 2: 왼쪽, 3: 오른쪽\n",
    "                real_action = self.action_dict[action]\n",
    "                # 죽었을 때 시작하기 위해 발사 행동을 함\n",
    "                if dead:\n",
    "                    action, real_action, dead = 0, 1, False\n",
    "\n",
    "                # 선택한 행동으로 환경에서 한 타임스텝 진행\n",
    "                observe, reward, done, info = self.env.step(real_action)\n",
    "\n",
    "                # 각 타임스텝마다 상태 전처리\n",
    "                next_state = pre_processing(observe)\n",
    "                next_state = np.reshape([next_state], (1, 84, 84, 1))\n",
    "                next_history = np.append(next_state, history[:, :, :, :3], axis=3)\n",
    "\n",
    "                # 정책확률의 최대값\n",
    "                self.avg_p_max += np.amax(policy.numpy())\n",
    "\n",
    "                if start_life > info['lives']:\n",
    "                    dead = True\n",
    "                    start_life = info['lives']\n",
    "\n",
    "                score += reward\n",
    "                reward = np.clip(reward, -1., 1.)\n",
    "\n",
    "                # 샘플을 저장\n",
    "                self.append_sample(history, action, reward)\n",
    "\n",
    "                if dead:\n",
    "                    history = np.stack((next_state, next_state,\n",
    "                                        next_state, next_state), axis=2)\n",
    "                    history = np.reshape([history], (1, 84, 84, 4))\n",
    "                else:\n",
    "                    history = next_history\n",
    "\n",
    "                # 에피소드가 끝나거나 최대 타임스텝 수에 도달하면 학습을 진행\n",
    "                if self.t >= self.t_max or done:\n",
    "                    self.train_model(done)\n",
    "                    self.t = 0\n",
    "\n",
    "                if done:\n",
    "                    # 각 에피소드 당 학습 정보를 기록\n",
    "                    episode += 1\n",
    "                    score_max = score if score > score_max else score_max\n",
    "                    score_avg = 0.9 * score_avg + 0.1 * score if score_avg != 0 else score\n",
    "\n",
    "                    log = \"episode: {:5d} | score : {:4.1f} | \".format(episode, score)\n",
    "                    log += \"score max : {:4.1f} | \".format(score_max)\n",
    "                    log += \"score avg : {:.3f}\".format(score_avg)\n",
    "                    print(log)\n",
    "\n",
    "                    self.draw_tensorboard(score, step, episode)\n",
    "\n",
    "                    self.avg_p_max = 0\n",
    "                    step = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9ccf169",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 학습속도를 높이기 위해 흑백화면으로 전처리\n",
    "def pre_processing(observe):\n",
    "    processed_observe = np.uint8(\n",
    "        resize(rgb2gray(observe), (84, 84), mode='constant') * 255)\n",
    "    return processed_observe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8b6b244",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    global_agent = A3CAgent(action_size=3, env_name=\"BreakoutDeterministic-v4\")\n",
    "    global_agent.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8152a2e5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "880b06b0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
