{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0701e631",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import gym\n",
    "import random\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from collections import deque\n",
    "\n",
    "from skimage.color import rgb2gray\n",
    "from skimage.transform import resize\n",
    "\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.layers import Conv2D, Dense, Flatten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fcd57aff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 상태가 입력, 큐함수가 출력인 인공신경망 생성\n",
    "class DQN(tf.keras.Model):\n",
    "    def __init__(self, action_size, state_size):\n",
    "        super(DQN, self).__init__()\n",
    "        self.conv1 = Conv2D(32, (8, 8), strides=(4, 4), activation='relu',\n",
    "                            input_shape=state_size)\n",
    "        self.conv2 = Conv2D(64, (4, 4), strides=(2, 2), activation='relu')\n",
    "        self.conv3 = Conv2D(64, (3, 3), strides=(1, 1), activation='relu')\n",
    "        self.flatten = Flatten()\n",
    "        self.fc = Dense(512, activation='relu')\n",
    "        self.fc_out = Dense(action_size)\n",
    "\n",
    "    def call(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.conv3(x)\n",
    "        x = self.flatten(x)\n",
    "        x = self.fc(x)\n",
    "        q = self.fc_out(x)\n",
    "        return q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1a083e6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 브레이크아웃 예제에서의 DQN 에이전트\n",
    "class DQNAgent:\n",
    "    def __init__(self, action_size, state_size=(84, 84, 4)):\n",
    "        self.render = False\n",
    "\n",
    "        # 상태와 행동의 크기 정의\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "\n",
    "        # DQN 하이퍼파라미터\n",
    "        self.discount_factor = 0.99\n",
    "        self.learning_rate = 1e-4\n",
    "        self.epsilon = 1.\n",
    "        self.epsilon_start, self.epsilon_end = 1.0, 0.02\n",
    "        self.exploration_steps = 1000000.\n",
    "        self.epsilon_decay_step = self.epsilon_start - self.epsilon_end\n",
    "        self.epsilon_decay_step /= self.exploration_steps\n",
    "        self.batch_size = 32\n",
    "        self.train_start = 50000\n",
    "        self.update_target_rate = 10000\n",
    "\n",
    "        # 리플레이 메모리, 최대 크기 100,000\n",
    "        self.memory = deque(maxlen=100000)\n",
    "        # 게임 시작 후 랜덤하게 움직이지 않는 것에 대한 옵션\n",
    "        self.no_op_steps = 30\n",
    "\n",
    "        # 모델과 타깃 모델 생성\n",
    "        self.model = DQN(action_size, state_size)\n",
    "        self.target_model = DQN(action_size, state_size)\n",
    "        self.optimizer = Adam(self.learning_rate, clipnorm=10.)\n",
    "        # 타깃 모델 초기화\n",
    "        self.update_target_model()\n",
    "\n",
    "        self.avg_q_max, self.avg_loss = 0, 0\n",
    "\n",
    "        self.writer = tf.summary.create_file_writer('summary/breakout_dqn')\n",
    "        self.model_path = os.path.join(os.getcwd(), 'save_model', 'model')\n",
    "\n",
    "    # 타깃 모델을 모델의 가중치로 업데이트\n",
    "    def update_target_model(self):\n",
    "        self.target_model.set_weights(self.model.get_weights())\n",
    "\n",
    "    # 입실론 탐욕 정책으로 행동 선택\n",
    "    def get_action(self, history):\n",
    "        history = np.float32(history / 255.0)\n",
    "        if np.random.rand() <= self.epsilon:\n",
    "            return random.randrange(self.action_size)\n",
    "        else:\n",
    "            q_value = self.model(history)\n",
    "            return np.argmax(q_value[0])\n",
    "\n",
    "    # 샘플 <s, a, r, s'>을 리플레이 메모리에 저장\n",
    "    def append_sample(self, history, action, reward, next_history, dead):\n",
    "        self.memory.append((history, action, reward, next_history, dead))\n",
    "\n",
    "    # 텐서보드에 학습 정보를 기록\n",
    "    def draw_tensorboard(self, score, step, episode):\n",
    "        with self.writer.as_default():\n",
    "            tf.summary.scalar('Total Reward/Episode', score, step=episode)\n",
    "            tf.summary.scalar('Average Max Q/Episode',\n",
    "                              self.avg_q_max / float(step), step=episode)\n",
    "            tf.summary.scalar('Duration/Episode', step, step=episode)\n",
    "            tf.summary.scalar('Average Loss/Episode',\n",
    "                              self.avg_loss / float(step), step=episode)\n",
    "\n",
    "    # 리플레이 메모리에서 무작위로 추출한 배치로 모델 학습\n",
    "    def train_model(self):\n",
    "        if self.epsilon > self.epsilon_end:\n",
    "            self.epsilon -= self.epsilon_decay_step\n",
    "\n",
    "        # 메모리에서 배치 크기만큼 무작위로 샘플 추출\n",
    "        batch = random.sample(self.memory, self.batch_size)\n",
    "\n",
    "        history = np.array([sample[0][0] / 255. for sample in batch],\n",
    "                           dtype=np.float32)\n",
    "        actions = np.array([sample[1] for sample in batch])\n",
    "        rewards = np.array([sample[2] for sample in batch])\n",
    "        next_history = np.array([sample[3][0] / 255. for sample in batch],\n",
    "                                dtype=np.float32)\n",
    "        dones = np.array([sample[4] for sample in batch])\n",
    "\n",
    "        # 학습 파라메터\n",
    "        model_params = self.model.trainable_variables\n",
    "        with tf.GradientTape() as tape:\n",
    "            # 현재 상태에 대한 모델의 큐함수\n",
    "            predicts = self.model(history)\n",
    "            one_hot_action = tf.one_hot(actions, self.action_size)\n",
    "            predicts = tf.reduce_sum(one_hot_action * predicts, axis=1)\n",
    "\n",
    "            # 다음 상태에 대한 타깃 모델의 큐함수\n",
    "            target_predicts = self.target_model(next_history)\n",
    "\n",
    "            # 벨만 최적 방정식을 구성하기 위한 타깃과 큐함수의 최대 값 계산\n",
    "            max_q = np.amax(target_predicts, axis=1)\n",
    "            targets = rewards + (1 - dones) * self.discount_factor * max_q\n",
    "\n",
    "            # 후버로스 계산\n",
    "            error = tf.abs(targets - predicts)\n",
    "            quadratic_part = tf.clip_by_value(error, 0.0, 1.0)\n",
    "            linear_part = error - quadratic_part\n",
    "            loss = tf.reduce_mean(0.5 * tf.square(quadratic_part) + linear_part)\n",
    "\n",
    "            self.avg_loss += loss.numpy()\n",
    "\n",
    "        # 오류함수를 줄이는 방향으로 모델 업데이트\n",
    "        grads = tape.gradient(loss, model_params)\n",
    "        self.optimizer.apply_gradients(zip(grads, model_params))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a708edb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 학습속도를 높이기 위해 흑백화면으로 전처리\n",
    "def pre_processing(observe):\n",
    "    processed_observe = np.uint8(\n",
    "        resize(rgb2gray(observe), (84, 84), mode='constant') * 255)\n",
    "    return processed_observe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c1541226",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'ale.lives'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-1187375933df>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     53\u001b[0m             \u001b[0magent\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mavg_q_max\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mamax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0magent\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhistory\u001b[0m \u001b[1;33m/\u001b[0m \u001b[1;36m255.\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     54\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 55\u001b[1;33m             \u001b[1;32mif\u001b[0m \u001b[0mstart_life\u001b[0m \u001b[1;33m>\u001b[0m \u001b[0minfo\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'ale.lives'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     56\u001b[0m                 \u001b[0mdead\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     57\u001b[0m                 \u001b[0mstart_life\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minfo\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'ale.lives'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'ale.lives'"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # 환경과 DQN 에이전트 생성\n",
    "    env = gym.make('BreakoutDeterministic-v4')\n",
    "    agent = DQNAgent(action_size=3)\n",
    "\n",
    "    global_step = 0\n",
    "    score_avg = 0\n",
    "    score_max = 0\n",
    "\n",
    "    # 불필요한 행동을 없애주기 위한 딕셔너리 선언\n",
    "    action_dict = {0:1, 1:2, 2:3, 3:3}\n",
    "\n",
    "    num_episode = 50000\n",
    "    for e in range(num_episode):\n",
    "        done = False\n",
    "        dead = False\n",
    "\n",
    "        step, score, start_life = 0, 0, 5\n",
    "        # env 초기화\n",
    "        observe = env.reset()\n",
    "\n",
    "        # 랜덤으로 뽑힌 값 만큼의 프레임동안 움직이지 않음\n",
    "        for _ in range(random.randint(1, agent.no_op_steps)):\n",
    "            observe, _, _, _ = env.step(1)\n",
    "\n",
    "        # 프레임을 전처리 한 후 4개의 상태를 쌓아서 입력값으로 사용.\n",
    "        state = pre_processing(observe)\n",
    "        history = np.stack((state, state, state, state), axis=2)\n",
    "        history = np.reshape([history], (1, 84, 84, 4))\n",
    "\n",
    "        while not done:\n",
    "            if agent.render:\n",
    "                env.render()\n",
    "            global_step += 1\n",
    "            step += 1\n",
    "\n",
    "            # 바로 전 history를 입력으로 받아 행동을 선택\n",
    "            action = agent.get_action(history)\n",
    "            # 1: 정지, 2: 왼쪽, 3: 오른쪽\n",
    "            real_action = action_dict[action]\n",
    "\n",
    "            # 죽었을 때 시작하기 위해 발사 행동을 함\n",
    "            if dead:\n",
    "                action, real_action, dead = 0, 1, False\n",
    "\n",
    "            # 선택한 행동으로 환경에서 한 타임스텝 진행\n",
    "            observe, reward, done, info = env.step(real_action)\n",
    "            # 각 타임스텝마다 상태 전처리\n",
    "            next_state = pre_processing(observe)\n",
    "            next_state = np.reshape([next_state], (1, 84, 84, 1))\n",
    "            next_history = np.append(next_state, history[:, :, :, :3], axis=3)\n",
    "\n",
    "            agent.avg_q_max += np.amax(agent.model(np.float32(history / 255.))[0])\n",
    "\n",
    "            if start_life > info['lives']:\n",
    "                dead = True\n",
    "                start_life = info['lives']\n",
    "\n",
    "            score += reward\n",
    "            reward = np.clip(reward, -1., 1.)\n",
    "            # 샘플 <s, a, r, s'>을 리플레이 메모리에 저장 후 학습\n",
    "            agent.append_sample(history, action, reward, next_history, dead)\n",
    "\n",
    "            # 리플레이 메모리 크기가 정해놓은 수치에 도달한 시점부터 모델 학습 시작\n",
    "            if len(agent.memory) >= agent.train_start:\n",
    "                agent.train_model()\n",
    "                # 일정 시간마다 타겟모델을 모델의 가중치로 업데이트\n",
    "                if global_step % agent.update_target_rate == 0:\n",
    "                    agent.update_target_model()\n",
    "\n",
    "            if dead:\n",
    "                history = np.stack((next_state, next_state,\n",
    "                                    next_state, next_state), axis=2)\n",
    "                history = np.reshape([history], (1, 84, 84, 4))\n",
    "            else:\n",
    "                history = next_history\n",
    "\n",
    "            if done:\n",
    "                # 각 에피소드 당 학습 정보를 기록\n",
    "                if global_step > agent.train_start:\n",
    "                    agent.draw_tensorboard(score, step, e)\n",
    "\n",
    "                score_avg = 0.9 * score_avg + 0.1 * score if score_avg != 0 else score\n",
    "                score_max = score if score > score_max else score_max\n",
    "\n",
    "                log = \"episode: {:5d} | \".format(e)\n",
    "                log += \"score: {:4.1f} | \".format(score)\n",
    "                log += \"score max : {:4.1f} | \".format(score_max)\n",
    "                log += \"score avg: {:4.1f} | \".format(score_avg)\n",
    "                log += \"memory length: {:5d} | \".format(len(agent.memory))\n",
    "                log += \"epsilon: {:.3f} | \".format(agent.epsilon)\n",
    "                log += \"q avg : {:3.2f} | \".format(agent.avg_q_max / float(step))\n",
    "                log += \"avg loss : {:3.2f}\".format(agent.avg_loss / float(step))\n",
    "                print(log)\n",
    "\n",
    "                agent.avg_q_max, agent.avg_loss = 0, 0\n",
    "\n",
    "        # 1000 에피소드마다 모델 저장\n",
    "        if e % 1000 == 0:\n",
    "            agent.model.save_weights(\"./save_model/model\", save_format=\"tf\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a041158",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20a6a823",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d093f56",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e82e9172",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
