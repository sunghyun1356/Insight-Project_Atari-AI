{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4e321c92",
   "metadata": {},
   "source": [
    "# train.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a6284b4b",
   "metadata": {},
   "outputs": [
    {
     "ename": "Error",
     "evalue": "We're Unable to find the game \"Breakout\". Note: Gym no longer distributes ROMs. If you own a license to use the necessary ROMs for research purposes you can download them via `pip install gym[accept-rom-license]`. Otherwise, you should try importing \"Breakout\" via the command `ale-import-roms`. If you believe this is a mistake perhaps your copy of \"Breakout\" is unsupported. To check if this is the case try providing the environment variable `PYTHONWARNINGS=default::ImportWarning:ale_py.roms`. For more information see: https://github.com/mgbellemare/Arcade-Learning-Environment#rom-management",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mError\u001b[0m                                     Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-93301bb2d0b3>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    155\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"__main__\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    156\u001b[0m     \u001b[1;31m# 환경과 DQN 에이전트 생성\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 157\u001b[1;33m     \u001b[0menv\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgym\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmake\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'BreakoutDeterministic-v4'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m##### 여기서 에러 뜸\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    158\u001b[0m     \u001b[0magent\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mDQNAgent\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maction_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    159\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\edu\\lib\\site-packages\\gym\\envs\\registration.py\u001b[0m in \u001b[0;36mmake\u001b[1;34m(id, **kwargs)\u001b[0m\n\u001b[0;32m    233\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    234\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mmake\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mid\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 235\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mregistry\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmake\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mid\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    236\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    237\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\edu\\lib\\site-packages\\gym\\envs\\registration.py\u001b[0m in \u001b[0;36mmake\u001b[1;34m(self, path, **kwargs)\u001b[0m\n\u001b[0;32m    127\u001b[0m             \u001b[0mlogger\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Making new env: %s\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    128\u001b[0m         \u001b[0mspec\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mspec\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 129\u001b[1;33m         \u001b[0menv\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mspec\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmake\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    130\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0menv\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    131\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\edu\\lib\\site-packages\\gym\\envs\\registration.py\u001b[0m in \u001b[0;36mmake\u001b[1;34m(self, **kwargs)\u001b[0m\n\u001b[0;32m     88\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     89\u001b[0m             \u001b[0mcls\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mentry_point\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 90\u001b[1;33m             \u001b[0menv\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcls\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0m_kwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     91\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     92\u001b[0m         \u001b[1;31m# Make the environment aware of which spec it came from.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\edu\\lib\\site-packages\\gym\\envs\\atari\\environment.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, game, mode, difficulty, obs_type, frameskip, repeat_action_probability, full_action_space, render_mode)\u001b[0m\n\u001b[0;32m    121\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    122\u001b[0m         \u001b[1;31m# Seed + Load\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 123\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mseed\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    124\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    125\u001b[0m         self._action_set = (\n",
      "\u001b[1;32m~\\anaconda3\\envs\\edu\\lib\\site-packages\\gym\\envs\\atari\\environment.py\u001b[0m in \u001b[0;36mseed\u001b[1;34m(self, seed)\u001b[0m\n\u001b[0;32m    170\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mroms\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_game\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    171\u001b[0m             raise error.Error(\n\u001b[1;32m--> 172\u001b[1;33m                 \u001b[1;34mf\"We're Unable to find the game \\\"{self._game}\\\". Note: Gym no longer distributes ROMs. \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    173\u001b[0m                 \u001b[1;34mf\"If you own a license to use the necessary ROMs for research purposes you can download them \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    174\u001b[0m                 \u001b[1;34mf\"via `pip install gym[accept-rom-license]`. Otherwise, you should try importing \\\"{self._game}\\\" \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mError\u001b[0m: We're Unable to find the game \"Breakout\". Note: Gym no longer distributes ROMs. If you own a license to use the necessary ROMs for research purposes you can download them via `pip install gym[accept-rom-license]`. Otherwise, you should try importing \"Breakout\" via the command `ale-import-roms`. If you believe this is a mistake perhaps your copy of \"Breakout\" is unsupported. To check if this is the case try providing the environment variable `PYTHONWARNINGS=default::ImportWarning:ale_py.roms`. For more information see: https://github.com/mgbellemare/Arcade-Learning-Environment#rom-management"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import gym\n",
    "import random\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from collections import deque\n",
    "\n",
    "from skimage.color import rgb2gray\n",
    "from skimage.transform import resize  ##### scikit-image로 install해야함\n",
    "\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.layers import Conv2D, Dense, Flatten\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# 상태가 입력, 큐함수가 출력인 인공신경망 생성\n",
    "class DQN(tf.keras.Model):\n",
    "    def __init__(self, action_size, state_size):\n",
    "        super(DQN, self).__init__()\n",
    "        self.conv1 = Conv2D(32, (8, 8), strides=(4, 4), activation='relu',\n",
    "                            input_shape=state_size)\n",
    "        self.conv2 = Conv2D(64, (4, 4), strides=(2, 2), activation='relu')\n",
    "        self.conv3 = Conv2D(64, (3, 3), strides=(1, 1), activation='relu')\n",
    "        self.flatten = Flatten()\n",
    "        self.fc = Dense(512, activation='relu')\n",
    "        self.fc_out = Dense(action_size)\n",
    "\n",
    "    def call(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.conv3(x)\n",
    "        x = self.flatten(x)\n",
    "        x = self.fc(x)\n",
    "        q = self.fc_out(x)\n",
    "        return q\n",
    "\n",
    "\n",
    "# 브레이크아웃 예제에서의 DQN 에이전트\n",
    "class DQNAgent:\n",
    "    def __init__(self, action_size, state_size=(84, 84, 4)):\n",
    "        self.render = False\n",
    "\n",
    "        # 상태와 행동의 크기 정의\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "\n",
    "        # DQN 하이퍼파라미터\n",
    "        self.discount_factor = 0.99\n",
    "        self.learning_rate = 1e-4\n",
    "        self.epsilon = 1.\n",
    "        self.epsilon_start, self.epsilon_end = 1.0, 0.02\n",
    "        self.exploration_steps = 1000000.\n",
    "        self.epsilon_decay_step = self.epsilon_start - self.epsilon_end\n",
    "        self.epsilon_decay_step /= self.exploration_steps\n",
    "        self.batch_size = 32\n",
    "        self.train_start = 50000\n",
    "        self.update_target_rate = 10000\n",
    "\n",
    "        # 리플레이 메모리, 최대 크기 100,000\n",
    "        self.memory = deque(maxlen=100000)\n",
    "        # 게임 시작 후 랜덤하게 움직이지 않는 것에 대한 옵션\n",
    "        self.no_op_steps = 30\n",
    "\n",
    "        # 모델과 타깃 모델 생성\n",
    "        self.model = DQN(action_size, state_size)\n",
    "        self.target_model = DQN(action_size, state_size)\n",
    "        self.optimizer = Adam(self.learning_rate, clipnorm=10.)\n",
    "        # 타깃 모델 초기화\n",
    "        self.update_target_model()\n",
    "\n",
    "        self.avg_q_max, self.avg_loss = 0, 0\n",
    "\n",
    "        self.writer = tf.summary.create_file_writer('summary/breakout_dqn')\n",
    "        self.model_path = os.path.join(os.getcwd(), 'save_model', 'model')\n",
    "\n",
    "    # 타깃 모델을 모델의 가중치로 업데이트\n",
    "    def update_target_model(self):\n",
    "        self.target_model.set_weights(self.model.get_weights())\n",
    "\n",
    "    # 입실론 탐욕 정책으로 행동 선택\n",
    "    def get_action(self, history):\n",
    "        history = np.float32(history / 255.0)\n",
    "        if np.random.rand() <= self.epsilon:\n",
    "            return random.randrange(self.action_size)\n",
    "        else:\n",
    "            q_value = self.model(history)\n",
    "            return np.argmax(q_value[0])\n",
    "\n",
    "    # 샘플 <s, a, r, s'>을 리플레이 메모리에 저장\n",
    "    def append_sample(self, history, action, reward, next_history, dead):\n",
    "        self.memory.append((history, action, reward, next_history, dead))\n",
    "\n",
    "    # 텐서보드에 학습 정보를 기록\n",
    "    def draw_tensorboard(self, score, step, episode):\n",
    "        with self.writer.as_default():\n",
    "            tf.summary.scalar('Total Reward/Episode', score, step=episode)\n",
    "            tf.summary.scalar('Average Max Q/Episode',\n",
    "                              self.avg_q_max / float(step), step=episode)\n",
    "            tf.summary.scalar('Duration/Episode', step, step=episode)\n",
    "            tf.summary.scalar('Average Loss/Episode',\n",
    "                              self.avg_loss / float(step), step=episode)\n",
    "\n",
    "    # 리플레이 메모리에서 무작위로 추출한 배치로 모델 학습\n",
    "    def train_model(self):\n",
    "        if self.epsilon > self.epsilon_end:\n",
    "            self.epsilon -= self.epsilon_decay_step\n",
    "\n",
    "        # 메모리에서 배치 크기만큼 무작위로 샘플 추출\n",
    "        batch = random.sample(self.memory, self.batch_size)\n",
    "\n",
    "        history = np.array([sample[0][0] / 255. for sample in batch],\n",
    "                           dtype=np.float32)\n",
    "        actions = np.array([sample[1] for sample in batch])\n",
    "        rewards = np.array([sample[2] for sample in batch])\n",
    "        next_history = np.array([sample[3][0] / 255. for sample in batch],\n",
    "                                dtype=np.float32)\n",
    "        dones = np.array([sample[4] for sample in batch])\n",
    "\n",
    "        # 학습 파라메터\n",
    "        model_params = self.model.trainable_variables\n",
    "        with tf.GradientTape() as tape:\n",
    "            # 현재 상태에 대한 모델의 큐함수\n",
    "            predicts = self.model(history)\n",
    "            one_hot_action = tf.one_hot(actions, self.action_size)\n",
    "            predicts = tf.reduce_sum(one_hot_action * predicts, axis=1)\n",
    "\n",
    "            # 다음 상태에 대한 타깃 모델의 큐함수\n",
    "            target_predicts = self.target_model(next_history)\n",
    "\n",
    "            # 벨만 최적 방정식을 구성하기 위한 타깃과 큐함수의 최대 값 계산\n",
    "            max_q = np.amax(target_predicts, axis=1)\n",
    "            targets = rewards + (1 - dones) * self.discount_factor * max_q\n",
    "\n",
    "            # 후버로스 계산\n",
    "            error = tf.abs(targets - predicts)\n",
    "            quadratic_part = tf.clip_by_value(error, 0.0, 1.0)\n",
    "            linear_part = error - quadratic_part\n",
    "            loss = tf.reduce_mean(0.5 * tf.square(quadratic_part) + linear_part)\n",
    "\n",
    "            self.avg_loss += loss.numpy()\n",
    "\n",
    "        # 오류함수를 줄이는 방향으로 모델 업데이트\n",
    "        grads = tape.gradient(loss, model_params)\n",
    "        self.optimizer.apply_gradients(zip(grads, model_params))\n",
    "\n",
    "\n",
    "# 학습속도를 높이기 위해 흑백화면으로 전처리\n",
    "def pre_processing(observe):\n",
    "    processed_observe = np.uint8(\n",
    "        resize(rgb2gray(observe), (84, 84), mode='constant') * 255)\n",
    "    return processed_observe\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # 환경과 DQN 에이전트 생성\n",
    "    env = gym.make('BreakoutDeterministic-v4') ##### 여기서 에러 뜸\n",
    "    agent = DQNAgent(action_size=3)\n",
    "\n",
    "    global_step = 0\n",
    "    score_avg = 0\n",
    "    score_max = 0\n",
    "\n",
    "    # 불필요한 행동을 없애주기 위한 딕셔너리 선언\n",
    "    action_dict = {0:1, 1:2, 2:3, 3:3}\n",
    "\n",
    "    num_episode = 50000\n",
    "    for e in range(num_episode):\n",
    "        done = False\n",
    "        dead = False\n",
    "\n",
    "        step, score, start_life = 0, 0, 5\n",
    "        # env 초기화\n",
    "        observe = env.reset()\n",
    "\n",
    "        # 랜덤으로 뽑힌 값 만큼의 프레임동안 움직이지 않음\n",
    "        for _ in range(random.randint(1, agent.no_op_steps)):\n",
    "            observe, _, _, _ = env.step(1)\n",
    "\n",
    "        # 프레임을 전처리 한 후 4개의 상태를 쌓아서 입력값으로 사용.\n",
    "        state = pre_processing(observe)\n",
    "        history = np.stack((state, state, state, state), axis=2)\n",
    "        history = np.reshape([history], (1, 84, 84, 4))\n",
    "\n",
    "        while not done:\n",
    "            if agent.render:\n",
    "                env.render()\n",
    "            global_step += 1\n",
    "            step += 1\n",
    "\n",
    "            # 바로 전 history를 입력으로 받아 행동을 선택\n",
    "            action = agent.get_action(history)\n",
    "            # 1: 정지, 2: 왼쪽, 3: 오른쪽\n",
    "            real_action = action_dict[action]\n",
    "\n",
    "            # 죽었을 때 시작하기 위해 발사 행동을 함\n",
    "            if dead:\n",
    "                action, real_action, dead = 0, 1, False\n",
    "\n",
    "            # 선택한 행동으로 환경에서 한 타임스텝 진행\n",
    "            observe, reward, done, info = env.step(real_action)\n",
    "            # 각 타임스텝마다 상태 전처리\n",
    "            next_state = pre_processing(observe)\n",
    "            next_state = np.reshape([next_state], (1, 84, 84, 1))\n",
    "            next_history = np.append(next_state, history[:, :, :, :3], axis=3)\n",
    "\n",
    "            agent.avg_q_max += np.amax(agent.model(np.float32(history / 255.))[0])\n",
    "\n",
    "            if start_life > info['ale.lives']:\n",
    "                dead = True\n",
    "                start_life = info['ale.lives']\n",
    "\n",
    "            score += reward\n",
    "            reward = np.clip(reward, -1., 1.)\n",
    "            # 샘플 <s, a, r, s'>을 리플레이 메모리에 저장 후 학습\n",
    "            agent.append_sample(history, action, reward, next_history, dead)\n",
    "\n",
    "            # 리플레이 메모리 크기가 정해놓은 수치에 도달한 시점부터 모델 학습 시작\n",
    "            if len(agent.memory) >= agent.train_start:\n",
    "                agent.train_model()\n",
    "                # 일정 시간마다 타겟모델을 모델의 가중치로 업데이트\n",
    "                if global_step % agent.update_target_rate == 0:\n",
    "                    agent.update_target_model()\n",
    "\n",
    "            if dead:\n",
    "                history = np.stack((next_state, next_state,\n",
    "                                    next_state, next_state), axis=2)\n",
    "                history = np.reshape([history], (1, 84, 84, 4))\n",
    "            else:\n",
    "                history = next_history\n",
    "\n",
    "            if done:\n",
    "                # 각 에피소드 당 학습 정보를 기록\n",
    "                if global_step > agent.train_start:\n",
    "                    agent.draw_tensorboard(score, step, e)\n",
    "\n",
    "                score_avg = 0.9 * score_avg + 0.1 * score if score_avg != 0 else score\n",
    "                score_max = score if score > score_max else score_max\n",
    "\n",
    "                log = \"episode: {:5d} | \".format(e)\n",
    "                log += \"score: {:4.1f} | \".format(score)\n",
    "                log += \"score max : {:4.1f} | \".format(score_max)\n",
    "                log += \"score avg: {:4.1f} | \".format(score_avg)\n",
    "                log += \"memory length: {:5d} | \".format(len(agent.memory))\n",
    "                log += \"epsilon: {:.3f} | \".format(agent.epsilon)\n",
    "                log += \"q avg : {:3.2f} | \".format(agent.avg_q_max / float(step))\n",
    "                log += \"avg loss : {:3.2f}\".format(agent.avg_loss / float(step))\n",
    "                print(log)\n",
    "\n",
    "                agent.avg_q_max, agent.avg_loss = 0, 0\n",
    "\n",
    "        # 1000 에피소드마다 모델 저장\n",
    "        if e % 1000 == 0:\n",
    "            agent.model.save_weights(\"./save_model/model\", save_format=\"tf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "43f1bf88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gym[atari] in c:\\users\\edr2\\anaconda3\\envs\\edu\\lib\\site-packages (0.21.0)\n",
      "Requirement already satisfied: numpy>=1.18.0 in c:\\users\\edr2\\anaconda3\\envs\\edu\\lib\\site-packages (from gym[atari]) (1.21.1)\n",
      "Requirement already satisfied: importlib-metadata>=4.8.1 in c:\\users\\edr2\\anaconda3\\envs\\edu\\lib\\site-packages (from gym[atari]) (4.8.2)\n",
      "Requirement already satisfied: cloudpickle>=1.2.0 in c:\\users\\edr2\\anaconda3\\envs\\edu\\lib\\site-packages (from gym[atari]) (2.0.0)\n",
      "Collecting ale-py~=0.7.1\n",
      "  Downloading ale_py-0.7.3-cp37-cp37m-win_amd64.whl (926 kB)\n",
      "Collecting importlib-resources\n",
      "  Downloading importlib_resources-5.4.0-py3-none-any.whl (28 kB)\n",
      "Requirement already satisfied: zipp>=0.5 in c:\\users\\edr2\\anaconda3\\envs\\edu\\lib\\site-packages (from importlib-metadata>=4.8.1->gym[atari]) (3.5.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.4 in c:\\users\\edr2\\anaconda3\\envs\\edu\\lib\\site-packages (from importlib-metadata>=4.8.1->gym[atari]) (3.10.0.0)\n",
      "Installing collected packages: importlib-resources, ale-py\n",
      "Successfully installed ale-py-0.7.3 importlib-resources-5.4.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -yproj (c:\\users\\edr2\\anaconda3\\envs\\edu\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -pype1 (c:\\users\\edr2\\anaconda3\\envs\\edu\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -illow (c:\\users\\edr2\\anaconda3\\envs\\edu\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -yproj (c:\\users\\edr2\\anaconda3\\envs\\edu\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -pype1 (c:\\users\\edr2\\anaconda3\\envs\\edu\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -illow (c:\\users\\edr2\\anaconda3\\envs\\edu\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -yproj (c:\\users\\edr2\\anaconda3\\envs\\edu\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -pype1 (c:\\users\\edr2\\anaconda3\\envs\\edu\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -illow (c:\\users\\edr2\\anaconda3\\envs\\edu\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -yproj (c:\\users\\edr2\\anaconda3\\envs\\edu\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -pype1 (c:\\users\\edr2\\anaconda3\\envs\\edu\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -illow (c:\\users\\edr2\\anaconda3\\envs\\edu\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -yproj (c:\\users\\edr2\\anaconda3\\envs\\edu\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -pype1 (c:\\users\\edr2\\anaconda3\\envs\\edu\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -illow (c:\\users\\edr2\\anaconda3\\envs\\edu\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -yproj (c:\\users\\edr2\\anaconda3\\envs\\edu\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -pype1 (c:\\users\\edr2\\anaconda3\\envs\\edu\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -illow (c:\\users\\edr2\\anaconda3\\envs\\edu\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -yproj (c:\\users\\edr2\\anaconda3\\envs\\edu\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -pype1 (c:\\users\\edr2\\anaconda3\\envs\\edu\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -illow (c:\\users\\edr2\\anaconda3\\envs\\edu\\lib\\site-packages)\n"
     ]
    }
   ],
   "source": [
    "!pip install gym[atari]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "79237e69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gym[accept-rom-license] in c:\\users\\edr2\\anaconda3\\envs\\edu\\lib\\site-packages (0.21.0)\n",
      "Requirement already satisfied: cloudpickle>=1.2.0 in c:\\users\\edr2\\anaconda3\\envs\\edu\\lib\\site-packages (from gym[accept-rom-license]) (2.0.0)\n",
      "Requirement already satisfied: numpy>=1.18.0 in c:\\users\\edr2\\anaconda3\\envs\\edu\\lib\\site-packages (from gym[accept-rom-license]) (1.21.1)\n",
      "Requirement already satisfied: importlib-metadata>=4.8.1 in c:\\users\\edr2\\anaconda3\\envs\\edu\\lib\\site-packages (from gym[accept-rom-license]) (4.8.2)\n",
      "Collecting autorom[accept-rom-license]~=0.4.2\n",
      "  Downloading AutoROM-0.4.2-py3-none-any.whl (16 kB)\n",
      "Requirement already satisfied: importlib-resources in c:\\users\\edr2\\anaconda3\\envs\\edu\\lib\\site-packages (from autorom[accept-rom-license]~=0.4.2->gym[accept-rom-license]) (5.4.0)\n",
      "Requirement already satisfied: click in c:\\users\\edr2\\anaconda3\\envs\\edu\\lib\\site-packages (from autorom[accept-rom-license]~=0.4.2->gym[accept-rom-license]) (7.1.2)\n",
      "Requirement already satisfied: requests in c:\\users\\edr2\\anaconda3\\envs\\edu\\lib\\site-packages (from autorom[accept-rom-license]~=0.4.2->gym[accept-rom-license]) (2.25.1)\n",
      "Requirement already satisfied: tqdm in c:\\users\\edr2\\anaconda3\\envs\\edu\\lib\\site-packages (from autorom[accept-rom-license]~=0.4.2->gym[accept-rom-license]) (4.62.2)\n",
      "Collecting AutoROM.accept-rom-license\n",
      "  Downloading AutoROM.accept-rom-license-0.4.2.tar.gz (9.8 kB)\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'done'\n",
      "  Preparing metadata (pyproject.toml): started\n",
      "  Preparing metadata (pyproject.toml): finished with status 'done'\n",
      "Requirement already satisfied: typing-extensions>=3.6.4 in c:\\users\\edr2\\anaconda3\\envs\\edu\\lib\\site-packages (from importlib-metadata>=4.8.1->gym[accept-rom-license]) (3.10.0.0)\n",
      "Requirement already satisfied: zipp>=0.5 in c:\\users\\edr2\\anaconda3\\envs\\edu\\lib\\site-packages (from importlib-metadata>=4.8.1->gym[accept-rom-license]) (3.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\edr2\\anaconda3\\envs\\edu\\lib\\site-packages (from requests->autorom[accept-rom-license]~=0.4.2->gym[accept-rom-license]) (2021.10.8)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in c:\\users\\edr2\\anaconda3\\envs\\edu\\lib\\site-packages (from requests->autorom[accept-rom-license]~=0.4.2->gym[accept-rom-license]) (4.0.0)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\edr2\\anaconda3\\envs\\edu\\lib\\site-packages (from requests->autorom[accept-rom-license]~=0.4.2->gym[accept-rom-license]) (1.26.6)\n",
      "Requirement already satisfied: idna<3,>=2.5 in c:\\users\\edr2\\anaconda3\\envs\\edu\\lib\\site-packages (from requests->autorom[accept-rom-license]~=0.4.2->gym[accept-rom-license]) (2.10)\n",
      "Requirement already satisfied: colorama in c:\\users\\edr2\\anaconda3\\envs\\edu\\lib\\site-packages (from tqdm->autorom[accept-rom-license]~=0.4.2->gym[accept-rom-license]) (0.4.4)\n",
      "Building wheels for collected packages: AutoROM.accept-rom-license\n",
      "  Building wheel for AutoROM.accept-rom-license (pyproject.toml): started\n",
      "  Building wheel for AutoROM.accept-rom-license (pyproject.toml): finished with status 'done'\n",
      "  Created wheel for AutoROM.accept-rom-license: filename=AutoROM.accept_rom_license-0.4.2-py3-none-any.whl size=441026 sha256=b1b60683bb922f4b56c6e27148f1726f4544eb29cae7822a975a2b6bf0a375e6\n",
      "  Stored in directory: c:\\users\\edr2\\appdata\\local\\pip\\cache\\wheels\\87\\67\\2e\\6147e7912fe37f5408b80d07527dab807c1d25f5c403a9538a\n",
      "Successfully built AutoROM.accept-rom-license\n",
      "Installing collected packages: AutoROM.accept-rom-license, autorom\n",
      "Successfully installed AutoROM.accept-rom-license-0.4.2 autorom-0.4.2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -yproj (c:\\users\\edr2\\anaconda3\\envs\\edu\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -pype1 (c:\\users\\edr2\\anaconda3\\envs\\edu\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -illow (c:\\users\\edr2\\anaconda3\\envs\\edu\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -yproj (c:\\users\\edr2\\anaconda3\\envs\\edu\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -pype1 (c:\\users\\edr2\\anaconda3\\envs\\edu\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -illow (c:\\users\\edr2\\anaconda3\\envs\\edu\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -yproj (c:\\users\\edr2\\anaconda3\\envs\\edu\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -pype1 (c:\\users\\edr2\\anaconda3\\envs\\edu\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -illow (c:\\users\\edr2\\anaconda3\\envs\\edu\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -yproj (c:\\users\\edr2\\anaconda3\\envs\\edu\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -pype1 (c:\\users\\edr2\\anaconda3\\envs\\edu\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -illow (c:\\users\\edr2\\anaconda3\\envs\\edu\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -yproj (c:\\users\\edr2\\anaconda3\\envs\\edu\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -pype1 (c:\\users\\edr2\\anaconda3\\envs\\edu\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -illow (c:\\users\\edr2\\anaconda3\\envs\\edu\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -yproj (c:\\users\\edr2\\anaconda3\\envs\\edu\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -pype1 (c:\\users\\edr2\\anaconda3\\envs\\edu\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -illow (c:\\users\\edr2\\anaconda3\\envs\\edu\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -yproj (c:\\users\\edr2\\anaconda3\\envs\\edu\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -pype1 (c:\\users\\edr2\\anaconda3\\envs\\edu\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -illow (c:\\users\\edr2\\anaconda3\\envs\\edu\\lib\\site-packages)\n"
     ]
    }
   ],
   "source": [
    "!pip install gym[accept-rom-license]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74a81711",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "edu",
   "language": "python",
   "name": "edu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
